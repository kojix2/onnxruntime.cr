<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Crystal Docs 1.15.1">
<meta name="crystal_docs.project_version" content="main">
<meta name="crystal_docs.project_name" content="onnxruntime">



<link href="css/style.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="js/doc.js"></script>

  <meta name="repository-name" content="onnxruntime">
  <title>onnxruntime main</title>
  <script type="text/javascript">
  CrystalDocs.base_path = "";
  </script>
</head>
<body>

<svg class="hidden">
  <symbol id="octicon-link" viewBox="0 0 16 16">
    <path fill="currentColor" fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
  </symbol>
</svg>
<input type="checkbox" id="sidebar-btn">
<label for="sidebar-btn" id="sidebar-btn-label">
  <svg class="open" xmlns="http://www.w3.org/2000/svg" height="2em" width="2em" viewBox="0 0 512 512"><title>Open Sidebar</title><path fill="currentColor" d="M80 96v64h352V96H80zm0 112v64h352v-64H80zm0 112v64h352v-64H80z"></path></svg>
  <svg class="close" xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" viewBox="0 0 512 512"><title>Close Sidebar</title><path fill="currentColor" d="m118.6 73.4-45.2 45.2L210.7 256 73.4 393.4l45.2 45.2L256 301.3l137.4 137.3 45.2-45.2L301.3 256l137.3-137.4-45.2-45.2L256 210.7Z"></path></svg>
</label>
<div class="sidebar">
  <div class="sidebar-header">
    <div class="search-box">
      <input type="search" class="search-input" placeholder="Search..." spellcheck="false" aria-label="Search">
    </div>

    <div class="project-summary">
      <h1 class="project-name">
        <a href="index.html">
          onnxruntime
        </a>
      </h1>

      <span class="project-version">
        main
      </span>
    </div>
  </div>

  <div class="search-results hidden">
    <ul class="search-list"></ul>
  </div>

  <div class="types-list">
    <ul>
  
  <li class="parent " data-id="onnxruntime/OnnxRuntime" data-name="onnxruntime">
      <a href="OnnxRuntime.html">OnnxRuntime</a>
      
        <ul>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/CpuProvider" data-name="onnxruntime::cpuprovider">
      <a href="OnnxRuntime/CpuProvider.html">CpuProvider</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/CpuProviderOptions" data-name="onnxruntime::cpuprovideroptions">
      <a href="OnnxRuntime/CpuProviderOptions.html">CpuProviderOptions</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/CudaProvider" data-name="onnxruntime::cudaprovider">
      <a href="OnnxRuntime/CudaProvider.html">CudaProvider</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/CudaProviderOptions" data-name="onnxruntime::cudaprovideroptions">
      <a href="OnnxRuntime/CudaProviderOptions.html">CudaProviderOptions</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/InferenceSession" data-name="onnxruntime::inferencesession">
      <a href="OnnxRuntime/InferenceSession.html">InferenceSession</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/IoBinding" data-name="onnxruntime::iobinding">
      <a href="OnnxRuntime/IoBinding.html">IoBinding</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/ModelMetadata" data-name="onnxruntime::modelmetadata">
      <a href="OnnxRuntime/ModelMetadata.html">ModelMetadata</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/NamedTensors" data-name="onnxruntime::namedtensors">
      <a href="OnnxRuntime/NamedTensors.html">NamedTensors</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/OrtEnvironment" data-name="onnxruntime::ortenvironment">
      <a href="OnnxRuntime/OrtEnvironment.html">OrtEnvironment</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/Provider" data-name="onnxruntime::provider">
      <a href="OnnxRuntime/Provider.html">Provider</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/ProviderOptions" data-name="onnxruntime::provideroptions">
      <a href="OnnxRuntime/ProviderOptions.html">ProviderOptions</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/RunOptions" data-name="onnxruntime::runoptions">
      <a href="OnnxRuntime/RunOptions.html">RunOptions</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/SparseTensor" data-name="onnxruntime::sparsetensor(t)">
      <a href="OnnxRuntime/SparseTensor.html">SparseTensor</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/SparseTensorFloat32" data-name="onnxruntime::sparsetensorfloat32">
      <a href="OnnxRuntime/SparseTensorFloat32.html">SparseTensorFloat32</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/SparseTensorFloat64" data-name="onnxruntime::sparsetensorfloat64">
      <a href="OnnxRuntime/SparseTensorFloat64.html">SparseTensorFloat64</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/SparseTensorInt32" data-name="onnxruntime::sparsetensorint32">
      <a href="OnnxRuntime/SparseTensorInt32.html">SparseTensorInt32</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/SparseTensorInt64" data-name="onnxruntime::sparsetensorint64">
      <a href="OnnxRuntime/SparseTensorInt64.html">SparseTensorInt64</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/Tensor" data-name="onnxruntime::tensor">
      <a href="OnnxRuntime/Tensor.html">Tensor</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/TensorElementDataType" data-name="onnxruntime::tensorelementdatatype">
      <a href="OnnxRuntime/TensorElementDataType.html">TensorElementDataType</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/TensorInfo" data-name="onnxruntime::tensorinfo">
      <a href="OnnxRuntime/TensorInfo.html">TensorInfo</a>
      
    </li>
  
  <li class=" " data-id="onnxruntime/OnnxRuntime/TrainingSession" data-name="onnxruntime::trainingsession">
      <a href="OnnxRuntime/TrainingSession.html">TrainingSession</a>
      
    </li>
  
</ul>

      
    </li>
  
</ul>

  </div>
</div>


<div class="main-content">
<h1><a id="onnxruntime.cr" class="anchor" href="#onnxruntime.cr">  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>onnxruntime.cr</h1>
<p><a href="https://github.com/kojix2/onnxruntime.cr/actions/workflows/test.yml"><img src="https://github.com/kojix2/onnxruntime.cr/actions/workflows/test.yml/badge.svg" alt="build" /></a>
<a href="https://tokei.kojix2.net/github/kojix2/onnxruntime.cr"><img src="https://img.shields.io/endpoint?url=https://tokei.kojix2.net/badge/github/kojix2/onnxruntime.cr/lines" alt="Lines of Code" /></a></p>
<p><a href="https://github.com/Microsoft/onnxruntime">ONNX Runtime</a> bindings for Crystal</p>
<h2><a id="installation" class="anchor" href="#installation">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Installation</h2>
<ol>
<li>
<p>Install ONNX Runtime</p>
<p>Download and install the ONNX Runtime from the <a href="https://github.com/microsoft/onnxruntime/releases">official releases</a>.</p>
<p>For Linux:</p>
<pre><code class="language-bash"># Example for Linux
wget https://github.com/microsoft/onnxruntime/releases/download/v1.21.0/onnxruntime-linux-x64-1.21.0.tgz
tar -xzf onnxruntime-linux-x64-1.21.0.tgz
export ONNXRUNTIME_DIR=/path/to/onnxruntime-linux-x64-1.21.0</code></pre>
<p>For macOS:</p>
<pre><code class="language-bash"># Example for macOS (arm64)
curl -L https://github.com/microsoft/onnxruntime/releases/download/v1.21.0/onnxruntime-osx-arm64-1.21.0.tgz -o onnxruntime-osx-arm64-1.21.0.tgz
tar -xzf onnxruntime-osx-arm64-1.21.0.tgz
export ONNXRUNTIME_DIR=/path/to/onnxruntime-osx-arm64-1.21.0</code></pre>
</li>
<li>
<p>Add the dependency to your <code>shard.yml</code>:</p>
<pre><code class="language-yaml">dependencies:
  onnxruntime:
    github: kojix2/onnxruntime.cr</code></pre>
</li>
<li>
<p>Run <code>shards install</code></p>
</li>
</ol>
<h2><a id="usage" class="anchor" href="#usage">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Usage</h2>
<pre><code class="language-crystal"><span class="k">require</span> <span class="s">&quot;onnxruntime&quot;</span>

<span class="c"># Load a model</span>
session <span class="o">=</span> <span class="t">OnnxRuntime</span><span class="t">::</span><span class="t">InferenceSession</span>.new(<span class="s">&quot;path/to/model.onnx&quot;</span>)

<span class="c"># Print model inputs and outputs</span>
puts <span class="s">&quot;Inputs:&quot;</span>
session.inputs.each <span class="k">do</span> <span class="o">|</span>input<span class="o">|</span>
  puts <span class="s">&quot;  </span><span class="i">#{</span>input.name<span class="i">}</span><span class="s">: </span><span class="i">#{</span>input.<span class="k">type</span><span class="i">}</span><span class="s"> </span><span class="i">#{</span>input.shape<span class="i">}</span><span class="s">&quot;</span>
<span class="k">end</span>

puts <span class="s">&quot;Outputs:&quot;</span>
session.outputs.each <span class="k">do</span> <span class="o">|</span>output<span class="o">|</span>
  puts <span class="s">&quot;  </span><span class="i">#{</span>output.name<span class="i">}</span><span class="s">: </span><span class="i">#{</span>output.<span class="k">type</span><span class="i">}</span><span class="s"> </span><span class="i">#{</span>output.shape<span class="i">}</span><span class="s">&quot;</span>
<span class="k">end</span>

<span class="c"># Prepare input data</span>
input_data <span class="o">=</span> {
  <span class="s">&quot;input_name&quot;</span> <span class="o">=&gt;</span> [<span class="n">1.0_f32</span>, <span class="n">2.0_f32</span>, <span class="n">3.0_f32</span>]
}

<span class="c"># Run inference</span>
result <span class="o">=</span> session.run(input_data)

<span class="c"># Process results</span>
result.each <span class="k">do</span> <span class="o">|</span>name, data<span class="o">|</span>
  puts <span class="s">&quot;</span><span class="i">#{</span>name<span class="i">}</span><span class="s">: </span><span class="i">#{</span>data<span class="i">}</span><span class="s">&quot;</span>
<span class="k">end</span>

<span class="c"># Explicitly release resources</span>
session.release_session
<span class="t">OnnxRuntime</span><span class="t">::</span><span class="t">InferenceSession</span>.release_env</code></pre>
<h2><a id="mnist-example" class="anchor" href="#mnist-example">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>MNIST Example</h2>
<p>Download the MNIST model: <a href="https://github.com/onnx/models/blob/main/validated/vision/classification/mnist/model/mnist-12.onnx">mnist-12.onnx</a> (<a href="https://github.com/onnx/models/raw/refs/heads/main/validated/vision/classification/mnist/model/mnist-12.onnx">raw</a></p>
<pre><code class="language-crystal"><span class="k">require</span> <span class="s">&quot;onnxruntime&quot;</span>

<span class="c"># Load the MNIST model</span>
session <span class="o">=</span> <span class="t">OnnxRuntime</span><span class="t">::</span><span class="t">InferenceSession</span>.new(<span class="s">&quot;mnist-12.onnx&quot;</span>)

<span class="c"># Create a dummy input (28x28 image draw 1)</span>
input_data <span class="o">=</span> <span class="t">Array</span>(<span class="t">Float32</span>).new(<span class="n">28</span> <span class="o">*</span> <span class="n">28</span>) { <span class="o">|</span>i<span class="o">|</span> (i <span class="o">%</span> <span class="n">14</span> <span class="o">==</span> <span class="n">0</span> ? <span class="n">1.0</span> : <span class="n">0.0</span>).to_f32 }

<span class="c"># Run inference</span>
result <span class="o">=</span> session.run({<span class="s">&quot;Input3&quot;</span> <span class="o">=&gt;</span> input_data}, [<span class="s">&quot;Plus214_Output_0&quot;</span>], shape: {<span class="s">&quot;Input3&quot;</span> <span class="o">=&gt;</span> [<span class="n">1_i64</span>, <span class="n">1_i64</span>, <span class="n">28_i64</span>, <span class="n">28_i64</span>]})

<span class="c"># Get the output probabilities</span>
probabilities <span class="o">=</span> result[<span class="s">&quot;Plus214_Output_0&quot;</span>].<span class="k">as</span>(<span class="t">Array</span>(<span class="t">Float32</span>))

<span class="c"># Find the digit with highest probability</span>
predicted_digit <span class="o">=</span> probabilities.index(probabilities.max)
puts <span class="s">&quot;Predicted digit: </span><span class="i">#{</span>predicted_digit<span class="i">}</span><span class="s">&quot;</span>

<span class="c"># Explicitly release resources</span>
session.release_session
<span class="t">OnnxRuntime</span><span class="t">::</span><span class="t">InferenceSession</span>.release_env</code></pre>
<h2><a id="memory-management" class="anchor" href="#memory-management">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Memory Management</h2>
<p>Currently, you must explicitly release resources when you're done with them:</p>
<pre><code class="language-crystal"><span class="c"># Create and use session</span>
session <span class="o">=</span> <span class="t">OnnxRuntime</span><span class="t">::</span><span class="t">InferenceSession</span>.new(<span class="s">&quot;path/to/model.onnx&quot;</span>)
result <span class="o">=</span> session.run(input_data)

<span class="c"># When finished, explicitly release resources</span>
session.release_session
<span class="t">OnnxRuntime</span><span class="t">::</span><span class="t">InferenceSession</span>.release_env</code></pre>
<p>For long-running applications like web servers, consider setting up signal handlers to ensure resources are properly released on shutdown:</p>
<pre><code class="language-crystal"><span class="t">Signal</span><span class="t">::</span><span class="t">INT</span>.trap <span class="k">do</span>
  puts <span class="s">&quot;Shutting down...&quot;</span>
  session.release_session
  <span class="t">OnnxRuntime</span><span class="t">::</span><span class="t">InferenceSession</span>.release_env
  exit
<span class="k">end</span></code></pre>
<p>See the examples directory for more detailed implementations.</p>
<p>Why do you need to manually free memory?</p>
<p>Previously, the following error was displayed on macOS.</p>
<pre><code class="language-crystal">libc++abi: terminating due to uncaught exception of type std::__1::system_error: mutex lock failed: Invalid argument
Program received and didn&#39;t handle signal ABRT (6)</code></pre>
<p>This seems to be related to multithreading and mutex. According to the AI, this is difficult to solve with <code>finalize</code>, so we tried to solve it by creating a reference counter, but we were unable to solve it in the end. If you can solve this problem, please create a pull request!</p>
<h2><a id="development" class="anchor" href="#development">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Development</h2>
<p>The code is generated by AI and may not be perfect.
Please feel free to contribute and improve it.</p>
<h2><a id="contributing" class="anchor" href="#contributing">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Contributing</h2>
<ol>
<li>Fork it (<a href="https://github.com/kojix2/onnxruntime.cr/fork">https://github.com/kojix2/onnxruntime.cr/fork</a>)</li>
<li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>
<li>Commit your changes (<code>git commit -am 'Add some feature'</code>)</li>
<li>Push to the branch (<code>git push origin my-new-feature</code>)</li>
<li>Create a new Pull Request</li>
</ol>
</div>
</body>
</html>
